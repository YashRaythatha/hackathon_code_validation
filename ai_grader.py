#!/usr/bin/env python3
"""
AI-Powered Hackathon Grader

This grader uses AI agents to evaluate hackathon submissions.
It's designed to be fair, consistent, and provide detailed feedback.

How it works:
1. User gives us a GitHub URL
2. We fetch the repo data
3. AI agents analyze different aspects
4. We combine everything into a final score
"""

import json
import sys
import time
from typing import Dict, List, Any, Optional
from github_analyzer import GitHubAnalyzer
from ai_agents import AgentOrchestrator


class AIGrader:
    """Main grader class - coordinates everything"""
    
    def __init__(self, github_token: Optional[str] = None, ai_api_key: Optional[str] = None):
        # Set up our GitHub connection and AI agents
        self.github = GitHubAnalyzer(github_token)
        self.agent_orchestrator = AgentOrchestrator(ai_api_key)
    
    def grade(self, repo_url: str, branch: str = 'main', commit_sha: Optional[str] = None,
              artifacts: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Main grading method - this is where the magic happens
        
        Args:
            repo_url: The GitHub repo we're analyzing
            branch: Which branch to look at (usually 'main')
            commit_sha: Specific commit if needed
            artifacts: Extra info the user might provide
        
        Returns:
            Complete grading results with scores and explanations
        """
        
        # Handle optional artifacts
        if artifacts is None:
            artifacts = {}
        
        try:
            print("ğŸ¤– Starting AI analysis...")
            print("-" * 60)
            
            # First, let's get the repo data from GitHub
            print("ğŸ“¡ Getting repository info from GitHub...")
            repo_info = self.github.get_repo_info(repo_url, branch)
            file_tree = self.github.get_file_tree(repo_info)
            readme = self.github.get_readme(repo_info)
            
            # Now we need to prepare everything for our AI agents
            print("ğŸ“¦ Setting up data for AI analysis...")
            context = {
                'file_tree': file_tree,
                'readme': readme,
                'artifacts': artifacts,
                'repo_info': repo_info,
                'repo_url': repo_url,
                'branch': branch,
                'commit_sha': commit_sha
            }
            
            # Time to let our AI agents do their thing
            print("ğŸ¤– Running AI agents...")
            print("   â€¢ Code Analysis Agent - Looking at code quality...")
            print("   â€¢ Architecture Agent - Checking project structure...")
            print("   â€¢ UI/UX Agent - Evaluating user interface...")
            print("   â€¢ Security Agent - Scanning for security issues...")
            print("   â€¢ Learning Agent - Using past experience...")
            
            ai_results = self.agent_orchestrator.analyze(context)
            
            # Now we need to turn the AI results into actual grades
            print("ğŸ“Š Converting AI analysis to scores...")
            final_results = self._map_ai_results_to_grades(ai_results, context)
            
            print("âœ… Analysis complete!\n")
            
            return final_results
            
        except Exception as e:
            return {
                "error": f"Failed to analyze repository: {str(e)}",
                "total_score": 0,
                "breakdown": {},
                "pass_fail": "fail",
                "ai_powered": True
            }
    
    def _map_ai_results_to_grades(self, ai_results: Dict[str, Any], 
                                  context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Map AI agent results to the grading structure
        
        AI Agent Mapping:
        - ui_ux agent â†’ UI category (10%)
        - architecture agent â†’ Architecture category (40%)
        - code agent â†’ Coding category (30%)
        - security agent â†’ Other/Compliance category (20%)
        """
        
        # Extract AI agent scores
        agent_scores = ai_results.get('agent_scores', {})
        agent_confidence = ai_results.get('confidence_scores', {})
        
        # Map agents to categories
        ui_score = agent_scores.get('ui_ux', 5)
        architecture_score = agent_scores.get('architecture', 5)
        coding_score = agent_scores.get('code', 5)
        other_score = agent_scores.get('security', 5)
        
        # Get evidence from AI agents
        all_evidence = ai_results.get('evidence', [])
        all_recommendations = ai_results.get('recommendations', [])
        all_insights = ai_results.get('insights', [])
        all_risks = ai_results.get('risks', [])
        
        # Separate evidence by agent/category
        ui_evidence = [e for e in all_evidence if 'ui_ux:' in e.lower()]
        arch_evidence = [e for e in all_evidence if 'architecture:' in e.lower()]
        code_evidence = [e for e in all_evidence if 'code:' in e.lower()]
        security_evidence = [e for e in all_evidence if 'security:' in e.lower()]
        
        # Calculate weighted total score
        weighted_total = round(
            ui_score * 0.10 +
            architecture_score * 0.40 +
            coding_score * 0.30 +
            other_score * 0.20
        )
        
        # Determine pass/fail status
        if weighted_total >= 7:
            pass_fail = "pass"
        elif weighted_total >= 5:
            pass_fail = "borderline"
        else:
            pass_fail = "fail"
        
        # Generate AI-powered explanations
        ui_comment = self._generate_ai_comment('UI/UX', ui_score, ui_evidence)
        arch_comment = self._generate_ai_comment('Architecture', architecture_score, arch_evidence)
        code_comment = self._generate_ai_comment('Coding', coding_score, code_evidence)
        other_comment = self._generate_ai_comment('Security/Compliance', other_score, security_evidence)
        
        # Build comprehensive result
        result = {
            "total_score": weighted_total,
            "breakdown": {
                "ui": {
                    "score": ui_score,
                    "weight": 10,
                    "evidence": ui_evidence[:10] if ui_evidence else ["AI analysis completed"],
                    "comment": ui_comment,
                    "ai_confidence": agent_confidence.get('ui_ux', 0.5)
                },
                "architecture": {
                    "score": architecture_score,
                    "weight": 40,
                    "evidence": arch_evidence[:10] if arch_evidence else ["AI analysis completed"],
                    "comment": arch_comment,
                    "ai_confidence": agent_confidence.get('architecture', 0.5)
                },
                "coding": {
                    "score": coding_score,
                    "weight": 30,
                    "evidence": code_evidence[:10] if code_evidence else ["AI analysis completed"],
                    "comment": code_comment,
                    "ai_confidence": agent_confidence.get('code', 0.5)
                },
                "other": {
                    "score": other_score,
                    "weight": 20,
                    "evidence": security_evidence[:10] if security_evidence else ["AI analysis completed"],
                    "comment": other_comment,
                    "ai_confidence": agent_confidence.get('security', 0.5),
                    "subchecks": self._generate_subchecks(other_score, security_evidence)
                }
            },
            "pass_fail": pass_fail,
            "ai_powered": True,
            "ai_insights": all_insights[:10],
            "ai_recommendations": all_recommendations[:10],
            "ai_risks": all_risks[:10],
            "top_strengths": self._generate_strengths(ui_score, architecture_score, coding_score, other_score),
            "top_improvements": self._generate_improvements(ui_score, architecture_score, coding_score, other_score),
            "prioritized_actions": self._generate_prioritized_actions(all_recommendations, all_risks),
            "plagiarism_flag": False,
            "risks_red_flags": all_risks[:5],
            "notes": {
                "assumptions": [
                    "All analysis performed by AI agents",
                    "No traditional rule-based analysis used",
                    "Confidence scores indicate AI reliability"
                ],
                "missing_artifacts": self._identify_missing_artifacts(context.get('artifacts', {})),
                "calculation": f"UI*0.10 + Architecture*0.40 + Coding*0.30 + Other*0.20 = {ui_score}*0.10 + {architecture_score}*0.40 + {coding_score}*0.30 + {other_score}*0.20 = {weighted_total}",
                "ai_agents_used": ai_results.get('agent_count', 5)
            }
        }
        
        return result
    
    def _generate_ai_comment(self, category: str, score: int, evidence: List[str]) -> str:
        """Generate AI-powered comment for a category"""
        if score >= 8:
            quality = "Excellent"
            desc = f"AI agents detected outstanding {category.lower()} quality with strong implementation"
        elif score >= 6:
            quality = "Good"
            desc = f"AI agents found solid {category.lower()} implementation with room for improvement"
        elif score >= 4:
            quality = "Fair"
            desc = f"AI agents identified basic {category.lower()} with several areas needing enhancement"
        else:
            quality = "Poor"
            desc = f"AI agents detected significant {category.lower()} issues requiring major improvements"
        
        evidence_summary = f" Based on {len(evidence)} pieces of evidence." if evidence else ""
        
        return f"{quality}: {desc}.{evidence_summary}"
    
    def _generate_subchecks(self, score: int, evidence: List[str]) -> List[Dict[str, Any]]:
        """Generate subchecks for Other category"""
        subchecks = []
        
        # Documentation
        doc_status = "pass" if score >= 6 else "partial" if score >= 4 else "fail"
        subchecks.append({
            "name": "documentation",
            "status": doc_status,
            "evidence": [e for e in evidence if 'document' in e.lower()][:2],
            "note": "AI-assessed documentation quality"
        })
        
        # Security
        sec_status = "pass" if score >= 6 else "partial" if score >= 4 else "fail"
        subchecks.append({
            "name": "security",
            "status": sec_status,
            "evidence": [e for e in evidence if 'security' in e.lower()][:2],
            "note": "AI-assessed security practices"
        })
        
        # Tests/CI
        test_status = "pass" if score >= 6 else "partial" if score >= 4 else "fail"
        subchecks.append({
            "name": "tests_ci",
            "status": test_status,
            "evidence": [e for e in evidence if 'test' in e.lower() or 'ci' in e.lower()][:2],
            "note": "AI-assessed testing and CI/CD"
        })
        
        # Licensing
        lic_status = "pass" if score >= 5 else "fail"
        subchecks.append({
            "name": "licensing",
            "status": lic_status,
            "evidence": [],
            "note": "AI-assessed licensing compliance"
        })
        
        # Accessibility/Performance
        a11y_status = "pass" if score >= 6 else "partial" if score >= 4 else "fail"
        subchecks.append({
            "name": "accessibility_or_performance",
            "status": a11y_status,
            "evidence": [],
            "note": "AI-assessed accessibility and performance"
        })
        
        # Originality
        subchecks.append({
            "name": "originality",
            "status": "pass",
            "evidence": [],
            "note": "AI-assessed originality"
        })
        
        return subchecks
    
    def _generate_strengths(self, ui_score: int, arch_score: int, 
                           coding_score: int, other_score: int) -> List[str]:
        """Generate top strengths based on AI scores"""
        strengths = []
        
        if arch_score >= 7:
            strengths.append("AI detected strong architecture and design patterns")
        if coding_score >= 7:
            strengths.append("AI identified high code quality and good practices")
        if ui_score >= 7:
            strengths.append("AI found excellent UI/UX implementation")
        if other_score >= 7:
            strengths.append("AI verified comprehensive documentation and compliance")
        
        if not strengths:
            strengths.append("AI analysis in progress - continue improving")
        
        return strengths[:3]
    
    def _generate_improvements(self, ui_score: int, arch_score: int,
                              coding_score: int, other_score: int) -> List[str]:
        """Generate top improvements based on AI scores"""
        improvements = []
        
        if ui_score < 5:
            improvements.append("AI recommends: Improve UI/UX design and accessibility")
        if arch_score < 5:
            improvements.append("AI recommends: Enhance architecture and separation of concerns")
        if coding_score < 5:
            improvements.append("AI recommends: Add tests and improve code quality")
        if other_score < 5:
            improvements.append("AI recommends: Improve documentation and compliance")
        
        if not improvements:
            improvements.append("AI suggests: Continue maintaining high quality standards")
        
        return improvements[:3]
    
    def _generate_prioritized_actions(self, recommendations: List[str], 
                                     risks: List[str]) -> List[Dict[str, Any]]:
        """Generate prioritized actions from AI recommendations"""
        actions = []
        priority = 1
        
        # High priority from risks
        for risk in risks[:2]:
            actions.append({
                "priority": priority,
                "action": f"Address: {risk}",
                "expected_impact": "AI-identified critical issue",
                "effort": "high"
            })
            priority += 1
        
        # Medium priority from recommendations
        for rec in recommendations[:3]:
            actions.append({
                "priority": priority,
                "action": rec,
                "expected_impact": "AI-recommended improvement",
                "effort": "medium"
            })
            priority += 1
        
        return actions[:5]
    
    def _identify_missing_artifacts(self, artifacts: Dict[str, Any]) -> List[str]:
        """Identify missing artifacts"""
        missing = []
        
        if not artifacts.get('test_results'):
            missing.append("test_results")
        if not artifacts.get('lint_results'):
            missing.append("lint_results")
        if not artifacts.get('screenshots_or_demo'):
            missing.append("screenshots_or_demo")
        if not artifacts.get('ci_config_present'):
            missing.append("ci_config")
        
        return missing


def display_results(results: Dict[str, Any], repo_url: str):
    """Show the grading results in a nice format"""
    print("\n" + "=" * 80)
    print("ğŸ“Š HACKATHON GRADING RESULTS")
    print("=" * 80)
    
    if "error" in results:
        print(f"\nâŒ Error: {results['error']}")
        return
    
    # Overall Score
    total_score = results.get('total_score', 0)
    pass_fail = results.get('pass_fail', 'fail')
    
    status_emoji = {'pass': 'âœ…', 'borderline': 'âš ï¸', 'fail': 'âŒ'}.get(pass_fail, 'â“')
    
    print(f"\nğŸ¯ OVERALL SCORE: {total_score}/10")
    print(f"ğŸ“ˆ STATUS: {status_emoji} {pass_fail.upper()}")
    print(f"ğŸ”— Repository: {repo_url}")
    
    # What the score means
    print(f"\nğŸ’¡ SCORE EXPLANATION:")
    if total_score >= 9:
        print("   ğŸŒŸ EXCELLENT: This is really impressive work!")
    elif total_score >= 7:
        print("   âœ… GOOD: Solid work that meets the standards")
    elif total_score >= 5:
        print("   âš ï¸  BORDERLINE: Not bad, but could use some improvements")
    else:
        print("   âŒ POOR: Needs quite a bit of work to get up to standard")
    
    # Category Breakdown
    print(f"\nğŸ“‹ DETAILED BREAKDOWN:")
    print("-" * 80)
    
    breakdown = results.get('breakdown', {})
    categories = [
        ('ui', 'UI/UX', 'ğŸ¨'),
        ('architecture', 'Architecture', 'ğŸ—ï¸'),
        ('coding', 'Coding', 'ğŸ’»'),
        ('other', 'Other/Compliance', 'ğŸ“‹')
    ]
    
    for category_key, category_name, emoji in categories:
        if category_key in breakdown:
            data = breakdown[category_key]
            score = data.get('score', 0)
            weight = data.get('weight', 0)
            confidence = data.get('ai_confidence', 0)
            comment = data.get('comment', '')
            evidence = data.get('evidence', [])
            
            print(f"\n{emoji} {category_name}: {score}/10 (Weight: {weight}%)")
            print(f"   ğŸ¤– AI Confidence: {confidence:.0%}")
            print(f"   ğŸ’¬ {comment}")
            
            if evidence:
                print(f"   ğŸ” Evidence:")
                for ev in evidence[:3]:
                    print(f"      â€¢ {ev}")
                if len(evidence) > 3:
                    print(f"      â€¢ ... and {len(evidence) - 3} more")
    
    # AI Insights
    if results.get('ai_insights'):
        print(f"\nğŸ’¡ AI INSIGHTS:")
        print("-" * 40)
        for i, insight in enumerate(results['ai_insights'][:5], 1):
            print(f"   {i}. {insight}")
    
    # AI Recommendations
    if results.get('ai_recommendations'):
        print(f"\nğŸ¯ AI RECOMMENDATIONS:")
        print("-" * 40)
        for i, rec in enumerate(results['ai_recommendations'][:5], 1):
            print(f"   {i}. {rec}")
    
    # Prioritized Actions
    if results.get('prioritized_actions'):
        print(f"\nğŸš€ PRIORITIZED ACTION ITEMS:")
        print("-" * 40)
        for action in results['prioritized_actions'][:3]:
            priority = action.get('priority', 1)
            action_text = action.get('action', '')
            effort = action.get('effort', 'medium')
            impact = action.get('expected_impact', '')
            
            effort_emoji = {'low': 'ğŸŸ¢', 'medium': 'ğŸŸ¡', 'high': 'ğŸ”´'}.get(effort, 'ğŸŸ¡')
            print(f"   {priority}. {action_text}")
            print(f"      {effort_emoji} Effort: {effort.title()} | ğŸ’¡ Impact: {impact}")
    
    # Risks
    if results.get('risks_red_flags'):
        print(f"\nğŸš¨ RISKS & RED FLAGS:")
        print("-" * 40)
        for risk in results['risks_red_flags'][:3]:
            print(f"   âš ï¸  {risk}")
    
    # Top Strengths
    if results.get('top_strengths'):
        print(f"\nâœ… TOP STRENGTHS:")
        for strength in results['top_strengths'][:3]:
            print(f"   â€¢ {strength}")
    
    # Top Improvements
    if results.get('top_improvements'):
        print(f"\nğŸ“ˆ TOP IMPROVEMENTS NEEDED:")
        for improvement in results['top_improvements'][:3]:
            print(f"   â€¢ {improvement}")
    
    # Calculation
    if results.get('notes', {}).get('calculation'):
        print(f"\nğŸ“Š SCORE CALCULATION:")
        print(f"   {results['notes']['calculation']}")
    
    print("\n" + "=" * 80)
    print("âœ… Evaluation Complete!")
    print("=" * 80 + "\n")


def main():
    """Command line interface for the grader"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AI-Powered Hackathon GitHub Submission Grader')
    parser.add_argument('--repo', required=True, help='GitHub repository URL')
    parser.add_argument('--branch', default='main', help='Branch to analyze (default: main)')
    parser.add_argument('--commit', help='Specific commit SHA to analyze')
    parser.add_argument('--artifacts', help='JSON file containing artifacts')
    parser.add_argument('--token', help='GitHub API token for private repos')
    parser.add_argument('--ai-key', help='AI API key for enhanced analysis')
    parser.add_argument('--output', help='Output file for results (optional)')
    parser.add_argument('--json', action='store_true', help='Output raw JSON instead of formatted display')
    
    args = parser.parse_args()
    
    # Load any additional artifacts if provided
    artifacts = {}
    if args.artifacts:
        try:
            with open(args.artifacts, 'r') as f:
                artifacts = json.load(f)
        except Exception as e:
            print(f"Warning: Could not load artifacts file: {e}")
    
    # Create our grader
    grader = AIGrader(args.token, args.ai_key)
    
    # Run the analysis
    results = grader.grade(args.repo, args.branch, args.commit, artifacts)
    
    # Show the results
    if args.json:
        # Just dump the raw JSON
        output = json.dumps(results, indent=2)
        print(output)
    else:
        # Pretty formatted output
        display_results(results, args.repo)
    
    # Save to file if they want it
    if args.output:
        with open(args.output, 'w') as f:
            f.write(json.dumps(results, indent=2))
        print(f"ğŸ“„ Results also saved to: {args.output}")


if __name__ == '__main__':
    main()

